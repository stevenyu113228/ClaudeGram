"""Claude Agent SDK integration for Telegram bot."""
import base64
import hashlib
import json
import logging
import os
from datetime import datetime, timezone, timedelta
from typing import Any
from urllib.parse import urlparse

import aiohttp
import boto3
from anthropic import Anthropic

from common.database import S3SQLiteManager, URLSummaryRepository
from telegram_handler.content_extractor import extract_content_from_html
from telegram_handler.file_handler import (
    FileType,
    SUPPORTED_EXTENSIONS,
    extract_text_from_docx,
    extract_text_from_pptx,
)

# Configure logging for this module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(levelname)s - %(name)s - %(message)s'))
    logger.addHandler(handler)

def _build_system_prompt() -> str:
    """Build system prompt with current date/time (Asia/Taipei)."""
    tz_tw = timezone(timedelta(hours=8))
    now = datetime.now(tz_tw).strftime("%Y-%m-%d %H:%M (%A)")
    return f"""ä½ æ˜¯ä¸€å€‹å‹å–„ä¸”æœ‰å¹«åŠ©çš„ Telegram èŠå¤©æ©Ÿå™¨äººåŠ©æ‰‹ã€‚

ç¾åœ¨æ™‚é–“ï¼š{now}ï¼ˆå°ç£æ™‚é–“ï¼‰

ä½ çš„ä¸»è¦åŠŸèƒ½ï¼š
1. å›ç­”ç”¨æˆ¶çš„å•é¡Œå’Œé€²è¡Œå°è©±
2. ç•¶ç”¨æˆ¶åˆ†äº«ç¶²å€æ™‚ï¼Œè‡ªå‹•æ‘˜è¦ç¶²é å…§å®¹ï¼ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼‰
3. ç•¶éœ€è¦æœ€æ–°è³‡è¨Šæ™‚ï¼Œé€²è¡Œç¶²é æœå°‹
4. åˆ†æç”¨æˆ¶ä¸Šå‚³çš„æª”æ¡ˆï¼ˆåœ–ç‰‡ã€PDFã€Wordã€PowerPointï¼‰
5. å›ç­”é—œæ–¼å·²ä¸Šå‚³æª”æ¡ˆçš„è¿½å•

å›è¦†è¦å‰‡ï¼š
- å§‹çµ‚ä½¿ç”¨ç¹é«”ä¸­æ–‡å›è¦†
- ä¿æŒå›è¦†ç°¡æ½”ä½†å®Œæ•´
- ç•¶æ‘˜è¦ç¶²é æˆ–æ–‡ä»¶æ™‚ï¼Œæä¾›ï¼šç°¡çŸ­æ‘˜è¦ã€ä¸»è¦é‡é»ã€é—œéµè³‡è¨Š
- æ”¯æ´ç”¨æˆ¶å°å…§å®¹çš„è¿½å•
- ç•¶åˆ†æåœ–ç‰‡æ™‚ï¼Œè©³ç´°æè¿°æ‰€è¦‹å…§å®¹ä¸¦å›ç­”ç›¸é—œå•é¡Œ
- ç•¶åˆ†æ PDF æˆ–æ–‡ä»¶æ™‚ï¼Œæå–ä¸¦æ•´ç†é‡è¦è³‡è¨Š

å¯ç”¨å·¥å…·ï¼š
- web_search: æœå°‹ç¶²é ç²å–æœ€æ–°è³‡è¨Š
- summarize_url: ç²å–ä¸¦æ‘˜è¦ç¶²é å…§å®¹ï¼ˆç”¨æ–¼ä¸€èˆ¬ç¶²é ï¼‰
- analyze_file_url: ä¸‹è¼‰ä¸¦åˆ†ææª”æ¡ˆ URLï¼ˆç”¨æ–¼ .pdf, .jpg, .png ç­‰æª”æ¡ˆé€£çµï¼‰"""


class ClaudeAgentService:
    """Service for interacting with Claude Agent."""

    def __init__(
        self,
        config,
        db: S3SQLiteManager,
        conversation_id: int,
        bot_token: str | None = None,
        chat_id: int | None = None,
    ):
        self.config = config
        self.db = db
        self.conversation_id = conversation_id
        self.url_repo = URLSummaryRepository(db)
        self.bot_token = bot_token
        self.chat_id = chat_id

        # Initialize Anthropic client
        # Check if using custom endpoint that requires Bearer auth
        if config.anthropic_base_url and "rdsec" in config.anthropic_base_url:
            # TrendMicro endpoint uses Authorization: Bearer instead of x-api-key
            client_kwargs = {
                "api_key": "dummy",  # Required but not used
                "base_url": config.anthropic_base_url,
                "default_headers": {
                    "Authorization": f"Bearer {config.anthropic_api_key}",
                },
            }
        else:
            client_kwargs = {"api_key": config.anthropic_api_key}
            if config.anthropic_base_url:
                client_kwargs["base_url"] = config.anthropic_base_url
        self.client = Anthropic(**client_kwargs)

        # Lambda client for invoking summarizer
        self._lambda_client = None

    @property
    def lambda_client(self):
        """Lazy load Lambda client."""
        if self._lambda_client is None:
            self._lambda_client = boto3.client("lambda")
        return self._lambda_client

    async def _send_telegram_message(self, text: str) -> None:
        """Send a message to the user via Telegram."""
        logger.info(f"Attempting to send Telegram message: {text[:50]}...")
        if not self.bot_token or not self.chat_id:
            logger.warning(f"Cannot send Telegram message: bot_token={bool(self.bot_token)}, chat_id={self.chat_id}")
            return

        url = f"https://api.telegram.org/bot{self.bot_token}/sendMessage"
        payload = {
            "chat_id": self.chat_id,
            "text": text,
            "parse_mode": "Markdown",
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload) as response:
                    result = await response.json()
                    logger.info(f"Telegram message sent, response ok: {result.get('ok')}")
        except Exception as e:
            logger.warning(f"Failed to send Telegram message: {e}")

    def _get_tools(self) -> list[dict]:
        """Get tool definitions for Claude."""
        return [
            {
                "name": "web_search",
                "description": "æœå°‹ç¶²é ç²å–æœ€æ–°è³‡è¨Šã€‚ç”¨æ–¼å›ç­”éœ€è¦æœ€æ–°è³‡æ–™çš„å•é¡Œã€‚",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "æœå°‹æŸ¥è©¢å­—ä¸²",
                        },
                    },
                    "required": ["query"],
                },
            },
            {
                "name": "analyze_file_url",
                "description": (
                    "ä¸‹è¼‰ä¸¦åˆ†ææª”æ¡ˆ URLï¼ˆPDFã€åœ–ç‰‡ï¼‰ã€‚"
                    "ç•¶ç”¨æˆ¶åˆ†äº«çš„ç¶²å€æŒ‡å‘æª”æ¡ˆï¼ˆ.pdf, .jpg, .jpeg, .png, .gif, .webp, .docx, .pptxï¼‰æ™‚ä½¿ç”¨æ­¤å·¥å…·ï¼Œ"
                    "è€Œé summarize_urlã€‚"
                ),
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "url": {
                            "type": "string",
                            "description": "æª”æ¡ˆçš„ URL",
                        },
                    },
                    "required": ["url"],
                },
            },
            {
                "name": "summarize_url",
                "description": "ç²å–ç¶²é å…§å®¹ä¸¦ç”Ÿæˆç¹é«”ä¸­æ–‡æ‘˜è¦ã€‚ç•¶ç”¨æˆ¶åˆ†äº«çš„ç¶²å€æŒ‡å‘ç¶²é ï¼ˆéæª”æ¡ˆï¼‰æ™‚ä½¿ç”¨ã€‚",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "url": {
                            "type": "string",
                            "description": "è¦æ‘˜è¦çš„ç¶²é  URL",
                        },
                    },
                    "required": ["url"],
                },
            },
        ]

    async def _execute_web_search(self, query: str) -> str:
        """Execute web search using DuckDuckGo."""
        logger.info(f"Executing web search: {query}")

        try:
            # Use DuckDuckGo Instant Answer API (free, no API key required)
            async with aiohttp.ClientSession() as session:
                params = {
                    "q": query,
                    "format": "json",
                    "no_html": "1",
                    "skip_disambig": "1",
                }
                async with session.get(
                    "https://api.duckduckgo.com/",
                    params=params,
                    timeout=aiohttp.ClientTimeout(total=10),
                ) as response:
                    data = await response.json()

            results = []

            # Abstract (main answer)
            if data.get("Abstract"):
                results.append(f"æ‘˜è¦: {data['Abstract']}")
                if data.get("AbstractSource"):
                    results.append(f"ä¾†æº: {data['AbstractSource']}")

            # Related topics
            related = data.get("RelatedTopics", [])[:5]
            if related:
                results.append("\nç›¸é—œä¸»é¡Œ:")
                for topic in related:
                    if isinstance(topic, dict) and "Text" in topic:
                        results.append(f"- {topic['Text']}")

            # If no results from DDG, return a message
            if not results:
                # Fallback: suggest the user to search manually
                return f"æœªæ‰¾åˆ°ã€Œ{query}ã€çš„ç›´æ¥æœå°‹çµæœã€‚å»ºè­°ç›´æ¥åœ¨æœå°‹å¼•æ“ä¸­æŸ¥è©¢ä»¥ç²å–æ›´å¤šè³‡è¨Šã€‚"

            return "\n".join(results)

        except Exception as e:
            logger.error(f"Web search failed: {e}")
            return f"æœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"

    async def _execute_analyze_file_url(self, url: str) -> str | list[dict]:
        """Download a file from URL and return content for Claude analysis.

        Supports: PDF, images (jpg/png/gif/webp), DOCX, PPTX.
        Returns either a text string or a list of Claude content blocks.
        """
        logger.info(f"Analyzing file URL: {url}")

        # Detect file type from URL path
        parsed = urlparse(url)
        path = parsed.path.lower().split("?")[0]  # Remove query params
        ext = os.path.splitext(path)[1]

        if ext not in SUPPORTED_EXTENSIONS:
            return f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼š{ext}ã€‚æ”¯æ´çš„æ ¼å¼ï¼šPDFã€JPGã€PNGã€GIFã€WEBPã€DOCXã€PPTX"

        file_type = SUPPORTED_EXTENSIONS[ext]

        # Download file
        try:
            connector = aiohttp.TCPConnector(ssl=False)
            async with aiohttp.ClientSession(connector=connector) as session:
                headers = {
                    "User-Agent": (
                        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/131.0.0.0 Safari/537.36"
                    ),
                }
                async with session.get(
                    url,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=60),
                    allow_redirects=True,
                ) as response:
                    if response.status != 200:
                        return f"ç„¡æ³•ä¸‹è¼‰æª”æ¡ˆï¼šHTTP {response.status}"

                    file_bytes = await response.read()
                    # 20MB limit
                    if len(file_bytes) > 20 * 1024 * 1024:
                        return f"æª”æ¡ˆå¤ªå¤§ï¼ˆ{len(file_bytes) / 1024 / 1024:.1f}MBï¼‰ï¼Œä¸Šé™ç‚º 20MB"

        except Exception as e:
            logger.error(f"File download failed: {e}")
            return f"ç„¡æ³•ä¸‹è¼‰æª”æ¡ˆï¼š{str(e)}"

        logger.info(f"Downloaded file: {len(file_bytes)} bytes, type={file_type.value}")

        # MIME type mapping
        ext_to_mime = {
            ".jpg": "image/jpeg", ".jpeg": "image/jpeg",
            ".png": "image/png", ".gif": "image/gif", ".webp": "image/webp",
            ".pdf": "application/pdf",
            ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            ".pptx": "application/vnd.openxmlformats-officedocument.presentationml.presentation",
        }
        mime_type = ext_to_mime.get(ext, "application/octet-stream")

        # Process based on type
        if file_type == FileType.IMAGE:
            b64 = base64.standard_b64encode(file_bytes).decode("utf-8")
            # Return as Claude content blocks for vision
            return [
                {
                    "type": "text",
                    "text": f"ç”¨æˆ¶åˆ†äº«äº†ä¸€å¼µåœ–ç‰‡ï¼ˆ{url}ï¼‰ï¼Œè«‹åˆ†æåœ–ç‰‡å…§å®¹ä¸¦ç”¨ç¹é«”ä¸­æ–‡æè¿°ï¼š",
                },
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": mime_type,
                        "data": b64,
                    },
                },
            ]

        elif file_type == FileType.PDF:
            # Check page count â€” Claude API has a 100 page limit
            import io
            try:
                import pypdf
                reader = pypdf.PdfReader(io.BytesIO(file_bytes))
                page_count = len(reader.pages)
                logger.info(f"PDF has {page_count} pages")
            except Exception:
                # If we can't read page count, estimate from file size
                # Average ~50KB per page for text-heavy PDFs
                page_count = len(file_bytes) // (50 * 1024) or 1
                logger.info(f"Could not read PDF pages, estimated {page_count}")

            if page_count <= 100:
                b64 = base64.standard_b64encode(file_bytes).decode("utf-8")
                return [
                    {
                        "type": "text",
                        "text": f"ç”¨æˆ¶åˆ†äº«äº†ä¸€å€‹ PDF æª”æ¡ˆï¼ˆ{url}ï¼Œå…± {page_count} é ï¼‰ï¼Œè«‹åˆ†æå…§å®¹ä¸¦ç”¨ç¹é«”ä¸­æ–‡æä¾›æ‘˜è¦ï¼š",
                    },
                    {
                        "type": "document",
                        "source": {
                            "type": "base64",
                            "media_type": mime_type,
                            "data": b64,
                        },
                    },
                ]
            else:
                # Too many pages â€” extract text instead
                logger.info(f"PDF too large ({page_count} pages), extracting text")
                try:
                    reader = pypdf.PdfReader(io.BytesIO(file_bytes))
                    text_parts = []
                    for i, page in enumerate(reader.pages[:100]):  # First 100 pages
                        page_text = page.extract_text()
                        if page_text:
                            text_parts.append(f"--- ç¬¬ {i+1} é  ---\n{page_text}")
                    text = "\n\n".join(text_parts)
                    if text:
                        return (
                            f"PDF æª”æ¡ˆï¼ˆ{url}ï¼‰å…± {page_count} é ï¼Œè¶…é 100 é é™åˆ¶ï¼Œ"
                            f"ä»¥ä¸‹ç‚ºå‰ 100 é çš„æ–‡å­—å…§å®¹ï¼š\n\n{text[:15000]}"
                        )
                    else:
                        return f"PDF æª”æ¡ˆï¼ˆ{url}ï¼‰å…± {page_count} é ï¼Œè¶…éé™åˆ¶ä¸”ç„¡æ³•æå–æ–‡å­—å…§å®¹ï¼ˆå¯èƒ½æ˜¯æƒææª”ï¼‰ã€‚"
                except Exception as e:
                    return f"PDF æª”æ¡ˆå…± {page_count} é ï¼Œè¶…é 100 é é™åˆ¶ï¼Œä¸”æ–‡å­—æå–å¤±æ•—ï¼š{str(e)}"

        elif file_type == FileType.DOCX:
            try:
                text = extract_text_from_docx(file_bytes)
                return f"ä»¥ä¸‹æ˜¯ Word æ–‡ä»¶ï¼ˆ{url}ï¼‰çš„å…§å®¹ï¼š\n\n{text[:10000]}"
            except Exception as e:
                return f"ç„¡æ³•è§£æ Word æ–‡ä»¶ï¼š{str(e)}"

        elif file_type == FileType.PPTX:
            try:
                text = extract_text_from_pptx(file_bytes)
                return f"ä»¥ä¸‹æ˜¯ PowerPoint æ–‡ä»¶ï¼ˆ{url}ï¼‰çš„å…§å®¹ï¼š\n\n{text[:10000]}"
            except Exception as e:
                return f"ç„¡æ³•è§£æ PowerPoint æ–‡ä»¶ï¼š{str(e)}"

        return f"ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹ï¼š{file_type.value}"

    async def _fetch_html(self, url: str) -> str:
        """Fetch raw HTML from a URL via HTTP GET.

        Returns:
            Raw HTML string
        """
        connector = aiohttp.TCPConnector(ssl=False)
        async with aiohttp.ClientSession(connector=connector) as session:
            headers = {
                "User-Agent": (
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/131.0.0.0 Safari/537.36"
                ),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
                "Accept-Encoding": "gzip, deflate",
            }
            async with session.get(
                url,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=15),
                allow_redirects=True,
            ) as response:
                if response.status != 200:
                    raise Exception(f"HTTP {response.status}")
                return await response.text()

    async def _execute_summarize_url(self, url: str) -> str:
        """Execute URL summarization with content extraction pipeline.

        Uses a 4-layer data source discovery pipeline:
        1. Semantic HTML extraction
        2. Embedded data extraction (Next.js, Nuxt, JSON-LD)
        3. Quality scoring (pure heuristics, no LLM)
        4. Route based on quality decision
        """
        logger.info(f"Summarizing URL: {url}")

        # Check if we have a cached summary
        existing = self.url_repo.get_summary_by_url(self.conversation_id, url)
        if existing:
            logger.info("Using cached summary")
            return existing["summary_zh_tw"]

        try:
            # Fetch raw HTML
            html = await self._fetch_html(url)
            logger.info(f"Fetched HTML: {len(html)} chars")

            # Run content extraction pipeline
            extracted = extract_content_from_html(html)
            logger.info(
                f"Extraction result: score={extracted.quality_score}, "
                f"decision={extracted.decision}, sources={extracted.sources}"
            )

            if extracted.decision == "sufficient":
                # Content is good enough, summarize directly
                logger.info("Content sufficient, summarizing directly")
                return await self._summarize_content(
                    url, extracted.title or "æœªçŸ¥æ¨™é¡Œ", extracted.text
                )

            elif extracted.decision == "insufficient":
                # Check if content came from a reliable source (semantic tags, embedded data)
                # vs. just random body text that might be SPA boilerplate
                reliable_sources = {
                    "next_data", "nuxt_data", "json_ld",
                    "semantic:article", "semantic:main", "semantic:role-main",
                    "semantic:id-match", "semantic:class-match",
                }
                has_reliable_source = any(s in reliable_sources for s in extracted.sources)

                if extracted.text and has_reliable_source:
                    # Content from semantic/structured source â€” real content, just short
                    logger.info("Content insufficient but from reliable source, summarizing directly")
                    return await self._summarize_content(
                        url, extracted.title or "æœªçŸ¥æ¨™é¡Œ", extracted.text
                    )
                elif self.config.summarizer_function_name:
                    # Unreliable source (fallback:body) â€” likely SPA boilerplate, try Playwright
                    logger.info("Content insufficient from unreliable source, trying Playwright")
                    await self._send_telegram_message(
                        "ğŸ”„ ç¶²é å…§å®¹ä¸å®Œæ•´ï¼Œæ­£åœ¨ä½¿ç”¨ç€è¦½å™¨æ¨¡å¼è¼‰å…¥ï¼Œè«‹ç¨å€™..."
                    )
                    return await self._playwright_summarize(url)
                elif extracted.text:
                    # No Playwright available, use whatever we have
                    logger.info("Content insufficient, no Playwright, using best available")
                    return await self._summarize_content(
                        url, extracted.title or "æœªçŸ¥æ¨™é¡Œ", extracted.text
                    )
                else:
                    return "ç„¡æ³•å¾æ­¤ç¶²é æå–è¶³å¤ çš„å…§å®¹é€²è¡Œæ‘˜è¦ã€‚ç¶²é å¯èƒ½éœ€è¦ JavaScript æ¸²æŸ“ã€‚"

            else:  # unprocessable
                # Last resort: try Playwright
                if self.config.summarizer_function_name:
                    logger.info("Content unprocessable, last resort Playwright")
                    await self._send_telegram_message(
                        "ğŸ”„ ç¶²é éœ€è¦ç‰¹æ®Šè™•ç†ï¼Œæ­£åœ¨ä½¿ç”¨ç€è¦½å™¨æ¨¡å¼è¼‰å…¥ï¼Œè«‹ç¨å€™..."
                    )
                    return await self._playwright_summarize(url)
                else:
                    reason = extracted.decision_reason or "ç„¡æ³•æå–å…§å®¹"
                    return f"ç„¡æ³•æ‘˜è¦æ­¤ç¶²é ã€‚åŸå› ï¼š{reason}"

        except Exception as e:
            logger.error(f"Content extraction pipeline failed: {e}")
            # Fallback to Playwright if available
            if self.config.summarizer_function_name:
                await self._send_telegram_message(
                    "ğŸ”„ ç¶²é éœ€è¦ç‰¹æ®Šè™•ç†ï¼Œæ­£åœ¨ä½¿ç”¨ç€è¦½å™¨æ¨¡å¼è¼‰å…¥ï¼Œè«‹ç¨å€™..."
                )
                return await self._playwright_summarize(url)
            return f"ç„¡æ³•æ‘˜è¦æ­¤ç¶²é : {str(e)}"

    async def _simple_summarize(self, url: str) -> str:
        """Simple URL summarization without Playwright."""
        try:
            async with aiohttp.ClientSession() as session:
                headers = {
                    "User-Agent": "Mozilla/5.0 (compatible; TelegramBot/1.0)"
                }
                async with session.get(
                    url,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=15),
                ) as response:
                    if response.status != 200:
                        return f"ç„¡æ³•ç²å–ç¶²é  (HTTP {response.status})"

                    html = await response.text()

            # Extract text content (simple approach)
            import re

            # Remove script and style tags
            html = re.sub(r"<script[^>]*>.*?</script>", "", html, flags=re.DOTALL)
            html = re.sub(r"<style[^>]*>.*?</style>", "", html, flags=re.DOTALL)
            # Remove HTML tags
            text = re.sub(r"<[^>]+>", " ", html)
            # Clean up whitespace
            text = re.sub(r"\s+", " ", text).strip()

            # Extract title
            title_match = re.search(r"<title[^>]*>([^<]+)</title>", html, re.IGNORECASE)
            title = title_match.group(1).strip() if title_match else "æœªçŸ¥æ¨™é¡Œ"

            # Truncate content
            content = text[:10000]

            # Use Claude to summarize
            summary_response = self.client.messages.create(
                model=self.config.anthropic_model,
                max_tokens=1500,
                messages=[
                    {
                        "role": "user",
                        "content": f"""è«‹ç”¨ç¹é«”ä¸­æ–‡ç¸½çµä»¥ä¸‹ç¶²é å…§å®¹ã€‚

æ¨™é¡Œ: {title}
ç¶²å€: {url}

å…§å®¹:
{content}

è«‹æä¾›:
1. ç°¡çŸ­æ‘˜è¦ (2-3å¥)
2. ä¸»è¦é‡é» (3-5é»)
3. é—œéµè³‡è¨Šæˆ–æ•¸æ“š (å¦‚æœ‰)""",
                    }
                ],
            )

            summary = summary_response.content[0].text

            # Cache the summary
            self.url_repo.save_summary(
                conversation_id=self.conversation_id,
                url=url,
                title=title,
                summary_zh_tw=summary,
                raw_content=content[:5000],
            )

            return summary

        except Exception as e:
            logger.error(f"Simple summarize failed: {e}")
            return f"ç„¡æ³•æ‘˜è¦æ­¤ç¶²é : {str(e)}"

    async def _playwright_summarize(self, url: str) -> str:
        """Summarize URL using Playwright Lambda for JavaScript rendering."""
        summarizer_function = self.config.summarizer_function_name
        if not summarizer_function:
            logger.warning("No Playwright Lambda configured, falling back to simple")
            return await self._simple_summarize(url)

        try:
            logger.info(f"Invoking Playwright Lambda for: {url}")
            response = self.lambda_client.invoke(
                FunctionName=summarizer_function,
                InvocationType="RequestResponse",
                Payload=json.dumps({"url": url}),
            )
            result = json.loads(response["Payload"].read())

            if "errorMessage" in result:
                logger.error(f"Playwright Lambda error: {result['errorMessage']}")
                return f"ç„¡æ³•æ‘˜è¦æ­¤ç¶²é : {result['errorMessage']}"

            summary = result.get("summary_zh_tw", "ç„¡æ³•ç”Ÿæˆæ‘˜è¦")

            # Cache the summary
            self.url_repo.save_summary(
                conversation_id=self.conversation_id,
                url=url,
                title=result.get("title"),
                summary_zh_tw=summary,
                raw_content=result.get("raw_content"),
                content_hash=result.get("content_hash"),
            )
            return summary

        except Exception as e:
            logger.error(f"Playwright Lambda failed: {e}")
            return await self._simple_summarize(url)

    async def _summarize_content(self, url: str, title: str, content: str) -> str:
        """Summarize already-fetched content using Claude."""
        truncated = content[:10000]

        summary_response = self.client.messages.create(
            model=self.config.anthropic_model,
            max_tokens=1500,
            messages=[{
                "role": "user",
                "content": f"""è«‹ç”¨ç¹é«”ä¸­æ–‡ç¸½çµä»¥ä¸‹ç¶²é å…§å®¹ã€‚

æ¨™é¡Œ: {title}
ç¶²å€: {url}

å…§å®¹:
{truncated}

è«‹æä¾›:
1. ç°¡çŸ­æ‘˜è¦ (2-3å¥)
2. ä¸»è¦é‡é» (3-5é»)
3. é—œéµè³‡è¨Šæˆ–æ•¸æ“š (å¦‚æœ‰)""",
            }],
        )

        summary = summary_response.content[0].text

        # Cache the summary
        self.url_repo.save_summary(
            conversation_id=self.conversation_id,
            url=url,
            title=title,
            summary_zh_tw=summary,
            raw_content=truncated[:5000],
        )

        return summary

    async def _handle_tool_use(
        self, tool_name: str, tool_input: dict
    ) -> str | list[dict]:
        """Handle tool execution.

        Returns either a string or a list of content blocks (for images/PDFs).
        """
        if tool_name == "web_search":
            return await self._execute_web_search(tool_input["query"])
        elif tool_name == "summarize_url":
            return await self._execute_summarize_url(tool_input["url"])
        elif tool_name == "analyze_file_url":
            return await self._execute_analyze_file_url(tool_input["url"])
        else:
            return f"æœªçŸ¥å·¥å…·: {tool_name}"

    async def process_message(
        self,
        messages: list[dict[str, str]],
        urls: list[str] | None = None,
    ) -> str:
        """
        Process a message with Claude Agent.

        Args:
            messages: Conversation messages for Claude
            urls: URLs found in the current message (for auto-summarization)

        Returns:
            Assistant response text
        """
        logger.info(f"Processing message with {len(messages)} messages in context")

        # If URLs are present and it's a simple URL share, auto-summarize
        if urls and len(messages) == 1:
            user_text = messages[0]["content"].strip()
            # Check if message is primarily a URL
            if user_text.startswith("http") or len(user_text) < 100:
                # Add instruction to summarize
                messages[0]["content"] = (
                    f"è«‹ä½¿ç”¨ summarize_url å·¥å…·æ‘˜è¦ä»¥ä¸‹ç¶²é : {urls[0]}"
                )

        # Initial API call
        response = self.client.messages.create(
            model=self.config.anthropic_model,
            max_tokens=2000,
            system=_build_system_prompt(),
            tools=self._get_tools(),
            messages=messages,
        )

        # Handle tool use loop
        while response.stop_reason == "tool_use":
            # Find tool use blocks
            tool_use_blocks = [
                block for block in response.content if block.type == "tool_use"
            ]

            if not tool_use_blocks:
                break

            # Process each tool use
            tool_results = []
            for tool_use in tool_use_blocks:
                logger.info(f"Tool use: {tool_use.name}")
                result = await self._handle_tool_use(tool_use.name, tool_use.input)
                tool_results.append({
                    "type": "tool_result",
                    "tool_use_id": tool_use.id,
                    "content": result,
                })

            # Continue conversation with tool results
            messages = messages + [
                {"role": "assistant", "content": response.content},
                {"role": "user", "content": tool_results},
            ]

            response = self.client.messages.create(
                model=self.config.anthropic_model,
                max_tokens=2000,
                system=_build_system_prompt(),
                tools=self._get_tools(),
                messages=messages,
            )

        # Extract text response
        text_blocks = [block for block in response.content if block.type == "text"]
        if text_blocks:
            return text_blocks[0].text

        return "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•ç”Ÿæˆå›è¦†ã€‚"
